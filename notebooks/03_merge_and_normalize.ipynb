{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 03. Merge & Normalize EUNIS Data\n",
        "\n",
        "**Goal:** Create a single, clean GeoDataFrame from the 20+ raw files.\n",
        "\n",
        "**Tasks:**\n",
        "1. Investigate `Code_tif` vs `Code_` (are they duplicates?).\n",
        "2. Define a **Column Mapping Strategy** to unify disparate names (e.g. `HabitatCod` -> `EUNIS_CODE`).\n",
        "3. Apply the mapping and merge all datasets.\n",
        "4. Save the result as a single Parquet or GeoPackage file (more efficient than GeoJSON)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing 8 files.\n"
          ]
        }
      ],
      "source": [
        "import geopandas as gpd\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Setup paths\n",
        "raw_dir = Path(\"../data/raw/eunis\")\n",
        "processed_dir = Path(\"../data/processed\")\n",
        "processed_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "files = sorted(list(raw_dir.glob(\"*.geojson\")))\n",
        "print(f\"Processing {len(files)} files.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Investigation: What is `Code_tif`?\n",
        "Let's load a dataset that contains both `Code_` and `Code_tif` (e.g., 'EUNIS_Forest_Distribution_point' usually has strictly structured attributes) to see if they carry the same information."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Checking correlation in: EUNIS_Forest_Distribution_point.geojson\n",
            "  Code_      Code_tif                                        Name_\n",
            "0   T11  T11_data.tif  Temperate Salix and Populus riparian forest\n",
            "1   T11  T11_data.tif  Temperate Salix and Populus riparian forest\n",
            "2   T11  T11_data.tif  Temperate Salix and Populus riparian forest\n",
            "3   T11  T11_data.tif  Temperate Salix and Populus riparian forest\n",
            "4   T11  T11_data.tif  Temperate Salix and Populus riparian forest\n",
            "5   T11  T11_data.tif  Temperate Salix and Populus riparian forest\n",
            "6   T11  T11_data.tif  Temperate Salix and Populus riparian forest\n",
            "7   T11  T11_data.tif  Temperate Salix and Populus riparian forest\n",
            "8   T11  T11_data.tif  Temperate Salix and Populus riparian forest\n",
            "9   T11  T11_data.tif  Temperate Salix and Populus riparian forest\n",
            "\n",
            "Unique types:\n",
            "Code_       str\n",
            "Code_tif    str\n",
            "dtype: object\n"
          ]
        }
      ],
      "source": [
        "# Load a sample file (ensure it's one that has Code_tif based on previous analysis)\n",
        "# We iterate until we find one with both columns\n",
        "sample_gdf = None\n",
        "target_file = None\n",
        "\n",
        "for p in files:\n",
        "    gdf_test = gpd.read_file(p, rows=5)\n",
        "    if 'Code_' in gdf_test.columns and 'Code_tif' in gdf_test.columns:\n",
        "        target_file = p\n",
        "        sample_gdf = gpd.read_file(p, rows=100) # Load 100 rows for check\n",
        "        break\n",
        "\n",
        "if sample_gdf is not None:\n",
        "    print(f\"Checking correlation in: {target_file.name}\")\n",
        "    print(sample_gdf[['Code_', 'Code_tif', 'Name_']].head(10))\n",
        "    \n",
        "    # Check if they match 1:1\n",
        "    # Usually Code_tif is an Integer ID, while Code_ is a String (e.g., T1.1)\n",
        "    print(\"\\nUnique types:\")\n",
        "    print(sample_gdf[['Code_', 'Code_tif']].dtypes)\n",
        "else:\n",
        "    print(\"No dataset found with both columns for comparison.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Decision on `Code_tif`:** # Usually, `Code_tif` is a raster grid ID (integer) used for spatial analysis, while `Code_` is the semantic label. \n",
        "\n",
        "*Strategy:* We will **keep both** but rename `Code_tif` to `RASTER_ID` to avoid confusion. The main analysis will use `EUNIS_CODE`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Define Normalization Logic\n",
        "We create a mapping dictionary. Any column not in this map (and not in `keep_columns`) will be dropped."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 1. Target Schema (Renaming)\n",
        "# Format: 'Original_Name': 'New_Unified_Name'\n",
        "column_mapping = {\n",
        "    # The Codes\n",
        "    'Code_': 'EUNIS_CODE',\n",
        "    'Code_2018': 'EUNIS_CODE',\n",
        "    'HabitatCod': 'EUNIS_HABITAT_CODE',  # Merging this dominant field here\n",
        "    \n",
        "    # The Names\n",
        "    'Name': 'EUNIS_NAME',\n",
        "    'Name_': 'EUNIS_NAME',\n",
        "    'Name_2018': 'EUNIS_NAME',\n",
        "    'Descriptio': 'EUNIS_DESC',\n",
        "    'Description': 'EUNIS_DESC',\n",
        "    \n",
        "    # Technical / Other\n",
        "    'Code_tif': 'RASTER_ID',\n",
        "    'Level': 'LEVEL',\n",
        "    'YEAR': 'YEAR',\n",
        "    'PRECISION': 'PRECISION_M', # Explicit unit\n",
        "    'PLOTOBSID': 'PLOT_ID'\n",
        "}\n",
        "\n",
        "# 2. Columns to always keep if they exist (don't rename, just keep)\n",
        "keep_columns = ['geometry', 'OBJECTID']\n",
        "\n",
        "def normalize_gdf(gdf, filename):\n",
        "    \"\"\"Renames columns and selects only the relevant subset.\"\"\"\n",
        "    \n",
        "    # 1. Rename columns based on mapping\n",
        "    # We use a loop because a file might have 'Code_' OR 'HabitatCod', \n",
        "    # and simple .rename() works for both mapping to the same target.\n",
        "    gdf = gdf.rename(columns=column_mapping)\n",
        "    \n",
        "    # 2. Add Source Metadata (Crucial for debugging later!)\n",
        "    gdf['SOURCE_FILE'] = filename\n",
        "    \n",
        "    # 3. Filter Columns\n",
        "    # Identify which of our target columns actually exist in this dataframe\n",
        "    target_cols = set(column_mapping.values())\n",
        "    existing_cols = set(gdf.columns)\n",
        "    \n",
        "    # Valid columns = (Renamed Targets) OR (Keep Columns)\n",
        "    cols_to_select = [c for c in existing_cols if c in target_cols or c in keep_columns or c == 'SOURCE_FILE']\n",
        "    \n",
        "    return gdf[cols_to_select]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Execute Merge\n",
        "We stream-load, normalize, and collect into a list (or merge iteratively if RAM is tight, but 20 files should fit in memory if they aren't massive)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Starting normalization...\n",
            "✅ Processed EUNIS_Coastal_Distribution_point.geojson | Rows: 34102\n",
            "✅ Processed EUNIS_Forest_Distribution_point.geojson | Rows: 220601\n",
            "✅ Processed EUNIS_Grassland_Distribution_point.geojson | Rows: 298801\n",
            "✅ Processed EUNIS_Hethland_Distribution_point.geojson | Rows: 42918\n",
            "✅ Processed EUNIS_Saltmarshes_Distribution_point.geojson | Rows: 21141\n",
            "✅ Processed EUNIS_Sparsely_Distribution_point.geojson | Rows: 6521\n",
            "✅ Processed EUNIS_Vegetated_Distribution_point.geojson | Rows: 79069\n",
            "✅ Processed EUNIS_Wetlands_Distribution_point.geojson | Rows: 92468\n",
            "\n",
            "Merging datasets...\n",
            "Final Dataset Shape: (795621, 12)\n",
            "CRS: EPSG:4326\n"
          ]
        }
      ],
      "source": [
        "normalized_dfs = []\n",
        "\n",
        "print(\"Starting normalization...\")\n",
        "for p in files:\n",
        "    try:\n",
        "        # Load\n",
        "        gdf_raw = gpd.read_file(p)\n",
        "        \n",
        "        # Normalize\n",
        "        gdf_clean = normalize_gdf(gdf_raw, p.name)\n",
        "        \n",
        "        # Fix CRS if needed (usually EPSG:3035 for EEA data, but verify)\n",
        "        if normalized_dfs and gdf_clean.crs != normalized_dfs[0].crs:\n",
        "            print(f\"⚠️ CRS Mismatch in {p.name}. Reprojecting...\")\n",
        "            gdf_clean = gdf_clean.to_crs(normalized_dfs[0].crs)\n",
        "            \n",
        "        normalized_dfs.append(gdf_clean)\n",
        "        print(f\"✅ Processed {p.name} | Rows: {len(gdf_clean)}\")\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"❌ Failed {p.name}: {e}\")\n",
        "\n",
        "# Concatenate all\n",
        "print(\"\\nMerging datasets...\")\n",
        "full_gdf = pd.concat(normalized_dfs, ignore_index=True)\n",
        "\n",
        "print(f\"Final Dataset Shape: {full_gdf.shape}\")\n",
        "print(f\"CRS: {full_gdf.crs}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Post-Merge Validation\n",
        "Let's check if we still have NaNs in the critical `EUNIS_CODE` column."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'pandas.DataFrame'>\n",
            "RangeIndex: 795621 entries, 0 to 795620\n",
            "Data columns (total 2 columns):\n",
            " #   Column      Non-Null Count   Dtype\n",
            "---  ------      --------------   -----\n",
            " 0   EUNIS_CODE  795621 non-null  str  \n",
            " 1   EUNIS_NAME  789996 non-null  str  \n",
            "dtypes: str(2)\n",
            "memory usage: 12.1 MB\n",
            "None\n",
            "\n",
            "✅ Success: All rows have a EUNIS_CODE.\n",
            "\n",
            "Habitat Levels: [nan  3.]\n"
          ]
        }
      ],
      "source": [
        "# Check for nulls in key fields\n",
        "print(full_gdf[['EUNIS_CODE', 'EUNIS_NAME']].info())\n",
        "\n",
        "missing_codes = full_gdf[full_gdf['EUNIS_CODE'].isnull()]\n",
        "if not missing_codes.empty:\n",
        "    print(f\"\\n⚠️ Warning: {len(missing_codes)} rows have no EUNIS_CODE!\")\n",
        "    print(missing_codes['SOURCE_FILE'].value_counts())\n",
        "else:\n",
        "    print(\"\\n✅ Success: All rows have a EUNIS_CODE.\")\n",
        "\n",
        "# Verify unique levels if the column exists\n",
        "if 'LEVEL' in full_gdf.columns:\n",
        "    print(\"\\nHabitat Levels:\", full_gdf['LEVEL'].unique())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Save Results\n",
        "We save to **Parquet** (fast, efficient for large data) or **GeoPackage**. \n",
        "Avoid GeoJSON for the merged file (it will be huge and slow)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved merged dataset to ../data/processed/eunis_merged_all.parquet\n"
          ]
        }
      ],
      "source": [
        "output_path = processed_dir / \"eunis_merged_all.parquet\"\n",
        "\n",
        "# Save as GeoParquet\n",
        "full_gdf.to_parquet(output_path)\n",
        "print(f\"Saved merged dataset to {output_path}\")\n",
        "\n",
        "# Optional: Save a small sample as GeoJSON for quick visualization in QGIS/GitHub\n",
        "full_gdf.sample(1000).to_file(processed_dir / \"eunis_sample_1k.geojson\", driver=\"GeoJSON\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "eunis-habitat-classification-LyFfyzSL",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
